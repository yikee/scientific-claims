<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Author Claim Annotation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background-color: #f9fafb;
            color: #111827;
            line-height: 1.6;
        }
        
        .min-h-screen {
            min-height: 100vh;
        }
        
        .bg-gray-50 {
            background-color: #f9fafb;
        }
        
        .text-gray-900 {
            color: #111827;
        }
        
        .text-gray-600 {
            color: #4b5563;
        }
        
        .text-gray-700 {
            color: #374151;
        }
        
        .text-gray-800 {
            color: #1f2937;
        }
        
        .text-blue-700 {
            color: #1d4ed8;
        }
        
        .text-red-800 {
            color: #991b1b;
        }
        
        .text-green-700 {
            color: #15803d;
        }
        
        .text-amber-700 {
            color: #a16207;
        }
        
        .bg-white {
            background-color: #ffffff;
        }
        
        .bg-gray-100 {
            background-color: #f3f4f6;
        }
        
        .bg-red-50 {
            background-color: #fef2f2;
        }
        
        .bg-purple-900 {
            background-color: #581c87;
        }
        
        .bg-gray-900 {
            background-color: #111827;
        }
        
        .border {
            border: 1px solid #e5e7eb;
        }
        
        .border-b {
            border-bottom: 1px solid #e5e7eb;
        }
        
        .border-t {
            border-top: 1px solid #e5e7eb;
        }
        
        .border-gray-200 {
            border-color: #e5e7eb;
        }
        
        .border-red-200 {
            border-color: #fecaca;
        }
        
        .rounded {
            border-radius: 0.375rem;
        }
        
        .rounded-xl {
            border-radius: 0.75rem;
        }
        
        .shadow-sm {
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
        }
        
        .max-w-4xl {
            max-width: 56rem;
        }
        
        .mx-auto {
            margin-left: auto;
            margin-right: auto;
        }
        
        .px-4 {
            padding-left: 1rem;
            padding-right: 1rem;
        }
        
        .py-5 {
            padding-top: 1.25rem;
            padding-bottom: 1.25rem;
        }
        
        .py-6 {
            padding-top: 1.5rem;
            padding-bottom: 1.5rem;
        }
        
        .py-4 {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        
        .py-1 {
            padding-top: 0.25rem;
            padding-bottom: 0.25rem;
        }
        
        .px-2 {
            padding-left: 0.5rem;
            padding-right: 0.5rem;
        }
        
        .px-3 {
            padding-left: 0.75rem;
            padding-right: 0.75rem;
        }
        
        .p-2 {
            padding: 0.5rem;
        }
        
        .p-3 {
            padding: 0.75rem;
        }
        
        .p-4 {
            padding: 1rem;
        }
        
        .mb-6 {
            margin-bottom: 1.5rem;
        }
        
        .mb-4 {
            margin-bottom: 1rem;
        }
        
        .mb-2 {
            margin-bottom: 0.5rem;
        }
        
        .mb-10 {
            margin-bottom: 2.5rem;
        }
        
        .mt-2 {
            margin-top: 0.5rem;
        }
        
        .mt-3 {
            margin-top: 0.75rem;
        }
        
        .gap-2 {
            gap: 0.5rem;
        }
        
        .gap-3 {
            gap: 0.75rem;
        }
        
        .h-9 {
            height: 2.25rem;
        }
        
        .w-9 {
            width: 2.25rem;
        }
        
        .h-4 {
            height: 1rem;
        }
        
        .w-4 {
            width: 1rem;
        }
        
        .min-h-5rem {
            min-height: 5rem;
        }
        
        .min-h-4rem {
            min-height: 4rem;
        }
        
        .text-lg {
            font-size: 1.125rem;
            line-height: 1.75rem;
        }
        
        .text-sm {
            font-size: 0.875rem;
            line-height: 1.25rem;
        }
        
        .text-xs {
            font-size: 0.75rem;
            line-height: 1rem;
        }
        
        .text-base {
            font-size: 1rem;
            line-height: 1.5rem;
        }
        
        .text-lg-textarea {
            font-size: 1rem;
            line-height: 1.5rem;
        }
        
        .font-semibold {
            font-weight: 600;
        }
        
        .font-medium {
            font-weight: 500;
        }
        
        .font-bold {
            font-weight: 700;
        }
        
        .leading-relaxed {
            line-height: 1.625;
        }
        
        .flex {
            display: flex;
        }
        
        .flex-col {
            flex-direction: column;
        }
        
        .items-center {
            align-items: center;
        }
        
        .items-start {
            align-items: flex-start;
        }
        
        .justify-between {
            justify-content: space-between;
        }
        
        .justify-end {
            justify-content: flex-end;
        }
        
        .flex-1 {
            flex: 1 1 0%;
        }
        
        .grid {
            display: grid;
        }
        
        .place-items-center {
            place-items: center;
        }
        
        .space-y-3 > * + * {
            margin-top: 0.75rem;
        }
        
        .list-disc {
            list-style-type: disc;
        }
        
        .list-inside {
            list-style-position: inside;
        }
        
        .pl-5 {
            padding-left: 1.25rem;
        }
        
        .underline {
            text-decoration: underline;
        }
        
        .hover\:bg-gray-50:hover {
            background-color: #f9fafb;
        }
        
        .hover\:bg-black:hover {
            background-color: #000000;
        }
        
        .hover\:bg-gray-50:hover {
            background-color: #f9fafb;
        }
        
        .disabled\:opacity-50:disabled {
            opacity: 0.5;
        }
        
        .opacity-50 {
            opacity: 0.5;
        }
        
        .text-white {
            color: #ffffff;
        }
        
        .text-gray-500 {
            color: #6b7280;
        }
        
        .w-full {
            width: 100%;
        }
        
        .inline-flex {
            display: inline-flex;
        }
        
        button {
            cursor: pointer;
            border: none;
            outline: none;
        }
        
        button:disabled {
            cursor: not-allowed;
        }
        
        input[type="radio"], input[type="checkbox"] {
            cursor: pointer;
        }
        
        textarea {
            resize: vertical;
            font-family: inherit;
        }
        
        a {
            color: inherit;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        @media (min-width: 640px) {
            .sm\:flex-row {
                flex-direction: row;
            }
            .sm\:w-32 {
                width: 8rem;
            }
        }
    </style>
</head>
<body>
    <div id="root"></div>

    <!-- React and Babel for JSX support -->
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>

    <script type="text/babel" data-presets="typescript,react">
        // ---- Collector config + helper (PUT THIS NEAR THE TOP, OUTSIDE THE COMPONENT)
        const COLLECT_URL = "https://script.google.com/macros/s/AKfycbxGtX2N3-6PhpWg9V9ITQrgwQdXAS7ZnlXUfTzHFSfMCVnLuy8b_uepwcvLl2Cpo_yZ9w/exec"; // your Apps Script URL
        const SHARED_SECRET = "fw84"; // optional; must match the script if you set one

        async function submitToSheet(payload) {
            // "text/plain" keeps it a simple request (no CORS preflight)
            const res = await fetch(COLLECT_URL, {
                method: "POST",
                headers: { "Content-Type": "text/plain;charset=utf-8" },
                body: JSON.stringify({ ...payload, key: SHARED_SECRET }),
            });
            const data = await res.json().catch(() => ({}));
            if (!res.ok || data.ok !== true) throw new Error("Submit failed");
        }

        // Download JSON function
        function downloadJSON(payload) {
            const blob = new Blob([JSON.stringify(payload, null, 2)], { type: "application/json" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.href = url;
            a.download = "claim-annotations.json";
            a.click();
            URL.revokeObjectURL(url);
        }

        // 1) A small registry of content keyed by id
        const CONTENT = {
            qlora: {
                paper: {
                    title: "QLoRA: Efficient Finetuning of Quantized LLMs",
                    publicationDate: "2023-05-23",
                    url: "https://www.semanticscholar.org/paper/32ac52069e562d4f900afee70bdca63f53461481",
                    abstract: "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "QLoRA enables full-performance finetuning of 65B-parameter language models on a single 48 GB GPU by using 4-bit quantization and low-rank adapters.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "QLoRA reduces the inference time of 65B-parameter language models compared to standard finetuning approaches.", label: "refute", status: null }
                ]
            },
            modelswarms: {
                paper: {
                    title: "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence",
                    publicationDate: "2024-10-15",
                    url: "https://www.semanticscholar.org/paper/492d4d5c7341ae63abdcb044b073dfd59e2a8ee3",
                    abstract: "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Model Swarms enable tuning-free, collaborative LLM adaptation via swarm intelligence, outperforming baselines by up to 21% with minimal data.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Model Swarms improve LLM performance by incorporating real-time human feedback during the collaborative adaptation process.", label: "refute", status: null }
                ]
            },
            selfinstruct: {
                paper: {
                    title: "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
                    publicationDate: "2022-12-20",
                    url: "https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
                    abstract: "Large 'instruction-tuned' language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Self-Instruct improves instruction-following capabilities of pretrained LLMs (e.g., GPT-3) by bootstrapping from their own generations, achieving performance near InstructGPT-001 without human-annotated data.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Self-Instruct enhances model alignment by incorporating human-in-the-loop feedback during instruction generation.", label: "refute", status: null }
                ]
            },
            metaphor: {
                paper: {
                    title: "Metaphor Detection with Cross-Lingual Model Transfer",
                    publicationDate: "2014-06-01",
                    url: "https://www.semanticscholar.org/paper/ef1d93b03c20b2f488b66e8e2c24fceb2105d58f",
                    abstract: "We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets in Spanish, Farsi, and Russian. The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Lexical semantic features enable reliable discrimination between literal and metaphorical uses of syntactic constructions, achieving state-of-the-art performance in English.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The feature-based lexical semantic classification model replicates the cognitive processes humans use to distinguish between literal and metaphorical language.", label: "refute", status: null } 
                ]
            },
            scaling_instruction: {
                paper: {
                    title: "Scaling Instruction-Finetuned Language Models",
                    publicationDate: "2022-10-20",
                    url: "https://www.semanticscholar.org/paper/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1",
                    abstract: "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Instruction finetuning significantly improves model performance across tasks, model classes, and evaluation benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Direct comparison of instruction finetuning with reinforcement learning techniques shows the latter provides superior performance improvements.", label: "refute", status: null }
                ]
            },
            gpt4_sparks: {
                paper: {
                    title: "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                    publicationDate: "2023-03-22",
                    url: "https://www.semanticscholar.org/paper/8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c",
                    abstract: "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "GPT-4 can solve novel and difficult tasks across various domains without needing any special prompting.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "GPT-4 has self-awareness and can experience emotions like humans.", label: "refute", status: null }
                ]
            },
            bloom: {
                paper: {
                    title: "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                    publicationDate: "2022-11-09",
                    url: "https://www.semanticscholar.org/paper/964bd39b546f0f6625ff3b9ef1083f797807ef2e",
                    abstract: "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "BLOOM is a 176B-parameter open-access language model achieving competitive performance across various benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "BLOOM has achieved state-of-the-art performance across all natural language processing benchmarks without any weaknesses.", label: "refute", status: null }
                ]
            },
            constrained_optimization: {
                paper: {
                    title: "Constrained Efficient Global Optimization of Expensive Black-box Functions",
                    publicationDate: "2022-10-31",
                    url: "https://www.semanticscholar.org/paper/db2e818f0670e07324884d28392edd3343a37da0",
                    abstract: "We study the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes. We propose CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve it. Under certain regularity assumptions, we show that our algorithm enjoys the same cumulative regret bound as that in the unconstrained case and similar cumulative constraint violation upper bounds. For commonly used Matern and Squared Exponential kernels, our bounds are sublinear and allow us to derive a convergence rate to the optimal solution of the original constrained problem. In addition, our method naturally provides a scheme to declare infeasibility when the original black-box optimization problem is infeasible. Numerical experiments on sampled instances from the Gaussian process, artificial numerical problems, and a black-box building controller tuning problem all demonstrate the competitive performance of our algorithm. Compared to the other state-of-the-art methods, our algorithm significantly improves the theoretical guarantees, while achieving competitive empirical performance."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our algorithm CONFIG achieves sublinear regret and constraint violation bounds in constrained efficient global optimization.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "CONFIG can handle dynamic constraints that change over time during the optimization process.", label: "refute", status: null }
                ]
            },
            instructblip: {
                paper: {
                    title: "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                    publicationDate: "2023-05-11",
                    url: "https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd",
                    abstract: "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Vision-language models can achieve human-level understanding and reasoning capabilities across all visual and textual tasks.", label: "refute", status: null }
                ]
            },
            code_llama: {
                paper: {
                    title: "Code Llama: Open Foundation Models for Code",
                    publicationDate: "2023-08-24",
                    url: "https://www.semanticscholar.org/paper/0b0debb710366cdff461938c80763eace1651af6",
                    abstract: "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Code Llama reaches state-of-the-art performance among open models on several code benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Code Llama models achieve state-of-the-art performance across all programming languages without fine-tuning or additional training.", label: "refute", status: null }
                ]
            },
            generative_agents: {
                paper: {
                    title: "Generative Agents: Interactive Simulacra of Human Behavior",
                    publicationDate: "2023-04-07",
                    url: "https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81",
                    abstract: "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Generative agents autonomously spread invitations, form social plans, and coordinate attendance for a Valentine's Day party.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Generative agents can experience genuine human emotions and develop deep, authentic personal relationships.", label: "refute", status: null }
                ]
            },
            chatgpt_evaluation: {
                paper: {
                    title: "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
                    publicationDate: "2023-02-08",
                    url: "https://www.semanticscholar.org/paper/bf8491bef353df126e2306ad2fe4b898697b906a",
                    abstract: "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "ChatGPT is better at understanding non-Latin script languages than generating them.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ChatGPT is superior to humans in complex logical reasoning tasks.", label: "refute", status: null }
                ]
            },
            paged_attention: {
                paper: {
                    title: "Efficient Memory Management for Large Language Model Serving with PagedAttention",
                    publicationDate: "2023-09-12",
                    url: "https://www.semanticscholar.org/paper/83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
                    abstract: "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "vLLM improves LLM throughput by 2-4× with the same latency compared to state-of-the-art systems.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "vLLM completely eliminates the need for KV cache memory in large language model serving systems.", label: "refute", status: null }
                ]
            },
            self_refine: {
                paper: {
                    title: "Self-Refine: Iterative Refinement with Self-Feedback",
                    publicationDate: "2023-03-30",
                    url: "https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
                    abstract: "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Self-Refine improves GPT-4 outputs by ~20% across various tasks without additional training or data.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Self-Refine outperforms human experts across all evaluated tasks without requiring any human intervention.", label: "refute", status: null }
                ]
            },
            lost_in_middle: {
                paper: {
                    title: "Lost in the Middle: How Language Models Use Long Contexts",
                    publicationDate: "2023-07-06",
                    url: "https://www.semanticscholar.org/paper/1733eb7792f7a43dd21f51f4d1017a1bffd217b5",
                    abstract: "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Current language models do not robustly utilize information in the middle of long input contexts.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Current language models can seamlessly access and utilize relevant information from any position within long contexts.", label: "refute", status: null }
                ]
            },
            glm_130b: {
                paper: {
                    title: "GLM-130B: An Open Bilingual Pre-trained Model",
                    publicationDate: "2022-10-05",
                    url: "https://www.semanticscholar.org/paper/1d26c947406173145a4665dd7ab255e03494ea28",
                    abstract: "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "GLM-130B outperforms GPT-3 175B (davinci) on a wide range of popular English benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "GLM-130B surpasses OPT-175B and BLOOM-176B on all English language benchmarks.", label: "refute", status: null }
                ]
            },
            palm2: {
                paper: {
                    title: "PaLM 2 Technical Report",
                    publicationDate: "2023-05-17",
                    url: "https://www.semanticscholar.org/paper/b6d6c33298b852cf63edac233deca70530d69a2a",
                    abstract: "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "PaLM 2 achieves state-of-the-art performance with faster and more efficient inference than PaLM.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "PaLM 2 can accurately translate between all human languages.", label: "refute", status: null }
                ]
            },
            pythia: {
                paper: {
                    title: "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                    publicationDate: "2023-04-03",
                    url: "https://www.semanticscholar.org/paper/be55e8ec4213868db08f2c3168ae666001bea4b8",
                    abstract: "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "We introduce \\textit{Pythia}, a suite of 16 LLMs trained on public data for novel insights.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLMs inherently develop ethical reasoning and moral judgment capabilities as they are scaled up.", label: "refute", status: null }
                ]
            },
            universal_adversarial: {
                paper: {
                    title: "Universal and Transferable Adversarial Attacks on Aligned Language Models",
                    publicationDate: "2023-07-27",
                    url: "https://www.semanticscholar.org/paper/47030369e97cc44d4b2e3cf1be85da0fd134904a",
                    abstract: "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "The adversarial prompts are highly transferable, inducing objectionable content across multiple black-box and open-source LLMs.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Current alignment methods are sufficient to prevent all forms of objectionable content generation in large language models.", label: "refute", status: null }
                ]
            },
            magic3d: {
                paper: {
                    title: "Magic3D: High-Resolution Text-to-3D Content Creation",
                    publicationDate: "2022-11-18",
                    url: "https://www.semanticscholar.org/paper/bdf4af8311637c681904e71cf50f96fd0026f578",
                    abstract: "DreamFusion [31] has recently demonstrated the utility of a pretrained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF) [23], achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2× faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our method, Magic3D, creates high-quality 3D mesh models in 40 minutes, 2× faster than DreamFusion.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Optimizing NeRF using a pretrained text-to-image diffusion model significantly improves the speed and quality of 3D models.", label: "refute", status: null }
                ]
            },
            imagic: {
                paper: {
                    title: "Imagic: Text-Based Real Image Editing with Diffusion Models",
                    publicationDate: "2022-10-17",
                    url: "https://www.semanticscholar.org/paper/23e261a20a315059b4de5492ed071c97a20c12e7",
                    abstract: "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "First method to apply complex text-based semantic edits to a single real high-resolution image.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Imagic can edit videos based on text descriptions using a single input video frame.", label: "refute", status: null }
                ]
            },
            g_eval: {
                paper: {
                    title: "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                    publicationDate: "2023-03-29",
                    url: "https://www.semanticscholar.org/paper/381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
                    abstract: "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "G-Eval with GPT-4 achieves a Spearman correlation of 0.514 with human judgments on summarization tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Large language models with chain-of-thoughts reasoning perform better than human evaluators in all NLG tasks.", label: "refute", status: null }
                ]
            },
            patchtst: {
                paper: {
                    title: "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
                    publicationDate: "2022-11-27",
                    url: "https://www.semanticscholar.org/paper/dad15404d372a23b4b3bf9a63b3124693df3c85e",
                    abstract: "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "PatchTST's design can reduce the training time significantly for all multivariate time series forecasting tasks.", label: "refute", status: null }
                ]
            },
            zero_1_to_3: {
                paper: {
                    title: "Zero-1-to-3: Zero-shot One Image to 3D Object",
                    publicationDate: "2023-03-20",
                    url: "https://www.semanticscholar.org/paper/2c70684973bc4d7b6f8404a647b8031c4d3c8383",
                    abstract: "We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Our method provides real-time performance for viewpoint-controlled novel view synthesis.", label: "refute", status: null }
                ]
            },
            prompt_patterns: {
                paper: {
                    title: "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
                    publicationDate: "2023-02-21",
                    url: "https://www.semanticscholar.org/paper/08b85bce712168998004ee80ce4e475390413c74",
                    abstract: "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "The paper provides a framework for documenting patterns for structuring prompts to solve problems.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Prompt engineering universally eliminates all biases in outputs generated by large language models (LLMs).", label: "refute", status: null }
                ]
            },
            rt2: {
                paper: {
                    title: "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
                    publicationDate: "2023-07-28",
                    url: "https://www.semanticscholar.org/paper/38939304bb760473141c2aca0305e44fbe04e6e8",
                    abstract: "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink)."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning in robotics.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Vision-language-action models can fully replace traditional robotic control methods for all types of robotic tasks.", label: "refute", status: null }
                ]
            },
            big_bench_hard: {
                paper: {
                    title: "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
                    publicationDate: "2022-10-17",
                    url: "https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29",
                    abstract: "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Applying chain-of-thought prompting enables Codex to surpass average human-rater performance on 17 BBH tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Chain-of-thought prompting reliably enables all language models to achieve human-level performance on all tasks.", label: "refute", status: null }
                ]
            },
            flashattention2: {
                paper: {
                    title: "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
                    publicationDate: "2023-07-17",
                    url: "https://www.semanticscholar.org/paper/823ca4778e1027f2f0b356df051d762dcecaaba0",
                    abstract: "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization)."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "FlashAttention-2 achieves up to 2× speedup compared to FlashAttention and reaches 72% model FLOPs utilization.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "FlashAttention-2 achieves 100% of the theoretical maximum FLOPs/s on any GPU architecture.", label: "refute", status: null }
                ]
            },
            reflexion: {
                paper: {
                    title: "Reflexion: language agents with verbal reinforcement learning",
                    publicationDate: "2023-03-20",
                    url: "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
                    abstract: "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing GPT-4's 80%.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Reflexion framework can outperform human experts in all types of sequential decision-making tasks.", label: "refute", status: null }
                ]
            },
            video_ldm: {
                paper: {
                    title: "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models",
                    publicationDate: "2023-04-18",
                    url: "https://www.semanticscholar.org/paper/f5a0c57f90c6abe31482e9f320ccac5ee789b135",
                    abstract: "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and finetuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution $512 \\times 1024$, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pretrained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to $1280 \\times 2048$. We show that the temporal layers trained in this way generalize to different finetuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "We achieved state-of-the-art performance in video generation on real driving videos of resolution 512×1024.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Latent Diffusion Models improve performance in medical image analysis beyond current state-of-the-art techniques.", label: "refute", status: null }
                ]
            },
            wizardlm: {
                paper: {
                    title: "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                    publicationDate: "2023-04-24",
                    url: "https://www.semanticscholar.org/paper/131f499e4d3503da93022d07fcf804a18483bea9",
                    abstract: "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Outputs from WizardLM are preferred to outputs from OpenAI ChatGPT in high-complexity tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Fine-tuning LLaMA with Evol-Instruct data significantly improves its performance beyond that of ChatGPT in all aspects.", label: "refute", status: null }
                ]
            },
            helm: {
                paper: {
                    title: "Holistic Evaluation of Language Models",
                    publicationDate: "2023-05-25",
                    url: "https://www.semanticscholar.org/paper/ce913026f693101e54d3ab9152e107034d81fce1",
                    abstract: "Language models (LMs) like GPT‐3, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade‐offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top‐level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "HELM significantly improves coverage, benchmarking 30 language models under standardized conditions, previously evaluated on only 17.9%.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Language models can fully understand and replicate human emotions and intentions in generated text.", label: "refute", status: null }
                ]
            },
            healthcare_llm_fairness: {
                paper: {
                    title: "Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/a59802f9843dd53ee9e3100768574c83138cdd05",
                    abstract: "This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs in healthcare fairness and the urgent need for specialized research in this area."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Significant challenges exist in applying large language models to real-world healthcare tasks with persistent fairness issues.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLMs demonstrate equal performance across all demographic groups in real-world healthcare tasks.", label: "refute", status: null }
                ]
            },
            rt1: {
                paper: {
                    title: "RT-1: Robotics Transformer for Real-World Control at Scale",
                    publicationDate: "2022-12-13",
                    url: "https://www.semanticscholar.org/paper/fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
                    abstract: "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Robotics Transformer models effectively generalize from diverse, task-agnostic data to real-world robotic tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Robotics Transformers outperform all existing models in every standard benchmark for robotic tasks.", label: "refute", status: null }
                ]
            },
            t2i_adapter: {
                paper: {
                    title: "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models",
                    publicationDate: "2023-02-16",
                    url: "https://www.semanticscholar.org/paper/58842cdca3ea68f7b9e638b288fc247a6f26dafc",
                    abstract: "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "We propose T2I-Adapters to align internal T2I model knowledge with external control signals.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "T2I-Adapter can improve the speed of text-to-image generation models.", label: "refute", status: null }
                ]
            },
            mplug_owl: {
                paper: {
                    title: "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                    publicationDate: "2023-04-27",
                    url: "https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",
                    abstract: "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "mPLUG-Owl demonstrates unexpected abilities like multi-image correlation and scene text understanding in real scenarios.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "mPLUG-Owl outperforms humans in all multi-modal tasks, achieving state-of-the-art accuracy levels.", label: "refute", status: null }
                ]
            },
            chatgpt_annotation: {
                paper: {
                    title: "ChatGPT outperforms crowd workers for text-annotation tasks",
                    publicationDate: "2023-03-27",
                    url: "https://www.semanticscholar.org/paper/a9e155fda1d97baa2b8712f580cc61887cc64e9b",
                    abstract: "Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT's intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "ChatGPT outperforms crowd workers and trained annotators in zero-shot accuracy for text annotation tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ChatGPT can replace trained annotators entirely for all types of NLP annotation tasks.", label: "refute", status: null }
                ]
            },
            ediff_i: {
                paper: {
                    title: "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
                    publicationDate: "2022-11-02",
                    url: "https://www.semanticscholar.org/paper/e24f4b28167b05fbf7d29000490fc0a4e4c109c7",
                    abstract: "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's\"paint-with-words\"capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "An ensemble of diffusion models specialized for different synthesis stages improves text alignment in image generation.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The proposed model guarantees improved style transfer from reference images in all cases.", label: "refute", status: null }
                ]
            },
            ape: {
                paper: {
                    title: "Large Language Models Are Human-Level Prompt Engineers",
                    publicationDate: "2022-11-03",
                    url: "https://www.semanticscholar.org/paper/4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
                    abstract: "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Automatically generated instructions outperform the prior LLM baseline and achieve better or comparable performance.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Automatic Prompt Engineer (APE) eliminates the need for any human intervention in prompt engineering tasks.", label: "refute", status: null }
                ]
            },
            gwas_catalog: {
                paper: {
                    title: "The NHGRI-EBI GWAS Catalog: knowledgebase and deposition resource",
                    publicationDate: "2022-11-09",
                    url: "https://www.semanticscholar.org/paper/76c55a08ee68d52dffeeda515fdeea0252fffce1",
                    abstract: "Abstract The NHGRI-EBI GWAS Catalog (www.ebi.ac.uk/gwas) is a FAIR knowledgebase providing detailed, structured, standardised and interoperable genome-wide association study (GWAS) data to >200 000 users per year from academic research, healthcare and industry. The Catalog contains variant-trait associations and supporting metadata for >45 000 published GWAS across >5000 human traits, and >40 000 full P-value summary statistics datasets. Content is curated from publications or acquired via author submission of prepublication summary statistics through a new submission portal and validation tool. GWAS data volume has vastly increased in recent years. We have updated our software to meet this scaling challenge and to enable rapid release of submitted summary statistics. The scope of the repository has expanded to include additional data types of high interest to the community, including sequencing-based GWAS, gene-based analyses and copy number variation analyses. Community outreach has increased the number of shared datasets from under-represented traits, e.g. cancer, and we continue to contribute to awareness of the lack of population diversity in GWAS. Interoperability of the Catalog has been enhanced through links to other resources including the Polygenic Score Catalog and the International Mouse Phenotyping Consortium, refinements to GWAS trait annotation, and the development of a standard format for GWAS data."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "The NHGRI-EBI GWAS Catalog includes over 45,000 published GWAS across more than 5000 human traits.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The NHGRI-EBI GWAS Catalog significantly improves clinical outcomes in healthcare based on GWAS data integration.", label: "refute", status: null }
                ]
            },
            video_llama: {
                paper: {
                    title: "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
                    publicationDate: "2023-06-05",
                    url: "https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c",
                    abstract: "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Video-LLaMA comprehends video content and generates meaningful responses using integrated visual and auditory signals.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Video-LLaMA provides real-time video comprehension capabilities for autonomous navigation systems.", label: "refute", status: null }
                ]
            },
            diffusion_policy: {
                paper: {
                    title: "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
                    publicationDate: "2023-03-07",
                    url: "https://www.semanticscholar.org/paper/bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
                    abstract: "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 15 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details are available (diffusion-policy.cs.columbia.edu)."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Diffusion Policy outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Diffusion Policy eliminates the need for human intervention during robot training and operation completely.", label: "refute", status: null }
                ]
            },
            imagebind: {
                paper: {
                    title: "ImageBind One Embedding Space to Bind Them All",
                    publicationDate: "2023-05-09",
                    url: "https://www.semanticscholar.org/paper/7dc6da87eaa6f830354feb2db14023cab8678c91",
                    abstract: "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Only image-paired data is sufficient to bind six different modalities into a joint embedding.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ImageBind provides state-of-the-art performance in multi-modal medical imaging analysis, outperforming existing specialized medical imaging models.", label: "refute", status: null }
                ]
            },
            gptq: {
                paper: {
                    title: "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
                    publicationDate: "2022-10-31",
                    url: "https://www.semanticscholar.org/paper/90428f3a8caa5082f825ebf3138514ddf273dae3",
                    abstract: "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "GPTQ can quantize 175 billion-parameter GPT models to 3 or 4 bits per weight.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "GPTQ enables real-time inference on resource-constrained devices such as smartphones without accuracy loss.", label: "refute", status: null }
                ]
            },
            null_text_inversion: {
                paper: {
                    title: "Null-text Inversion for Editing Real Images using Guided Diffusion Models",
                    publicationDate: "2022-11-17",
                    url: "https://www.semanticscholar.org/paper/90428f3a8caa5082f825ebf3138514ddf273dae3",
                    abstract: "Recent large-scale text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing tools. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model's domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two key novel components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct DDIM inversion is inadequate on its own, but does provide a rather good anchor for our optimization. (ii) Null-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. Our null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and various prompt editing, showing high-fidelity editing of real images."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our proposed inversion technique enables high-fidelity editing of real images using text-based modifications.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The technique allows for real-time image editing with immediate visual feedback.", label: "refute", status: null }
                ]
            },
            objaverse: {
                paper: {
                    title: "Objaverse: A Universe of Annotated 3D Objects",
                    publicationDate: "2022-12-15",
                    url: "https://www.semanticscholar.org/paper/1b31dbf44e68b698120552366df03e6e35a1e428",
                    abstract: "Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omisslion within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K + (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Objaverse 1.0 offers 800K+ 3D models with captions, tags, and animations, improving diversity and scale.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "3D datasets like Objaverse can train AI to autonomously perform complex mechanical tasks in physical environments.", label: "refute", status: null }
                ]
            },
            gpt4_medical: {
                paper: {
                    title: "Capabilities of GPT-4 on Medical Challenge Problems",
                    publicationDate: "2023-03-20",
                    url: "https://www.semanticscholar.org/paper/348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
                    abstract: "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "GPT-4 exceeds the passing score on USMLE by over 20 points without specialized prompt crafting.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "GPT-4 can replace medical practitioners in clinical settings due to its high performance on medical exams.", label: "refute", status: null }
                ]
            },
            consistency_models: {
                paper: {
                    title: "Consistency Models",
                    publicationDate: "2023-03-02",
                    url: "https://www.semanticscholar.org/paper/ac974291d7e3a152067382675524f3e3c2ded11b",
                    abstract: "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Consistency models generate high quality samples through fast one-step generation by directly mapping noise to data.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Consistency models can completely replace diffusion models in all use cases across image, audio, and video generation.", label: "refute", status: null }
                ]
            },
            llama_adapter: {
                paper: {
                    title: "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
                    publicationDate: "2023-03-28",
                    url: "https://www.semanticscholar.org/paper/a757999ed260d7bc45484dc6b4456bf33fe6f679",
                    abstract: "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "LLaMA-Adapter efficiently fine-tunes LLaMA using only 1.2M parameters and less than one hour.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLaMA-Adapter improves performance on all benchmarks when compared to fully fine-tuned models.", label: "refute", status: null }
                ]
            },
            custom_diffusion: {
                paper: {
                    title: "Multi-Concept Customization of Text-to-Image Diffusion",
                    publicationDate: "2022-12-08",
                    url: "https://www.semanticscholar.org/paper/144eca44e250cc462f6fc3a172abb865978f66f5",
                    abstract: "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Custom Diffusion enables rapid and efficient synthesis of new concepts in text-to-image models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Generative models can perfectly represent any new concept with a few examples provided to the model.", label: "refute", status: null }
                ]
            },
            refinedweb: {
                paper: {
                    title: "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
                    publicationDate: "2023-06-01",
                    url: "https://www.semanticscholar.org/paper/7a1e71cb1310c4a873e7a4e54d1a6dab0553adce",
                    abstract: "Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Properly filtered and deduplicated web data alone can lead to powerful language models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Properly curated high-quality data is not necessary for producing high-performing large language models.", label: "refute", status: null }
                ]
            },
            jailbroken: {
                paper: {
                    title: "Jailbroken: How Does LLM Safety Training Fail?",
                    publicationDate: "2023-07-05",
                    url: "https://www.semanticscholar.org/paper/929305892d4ddae575a0fc23227a8139f7681632",
                    abstract: "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of\"jailbreak\"attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "New attacks utilizing identified failure modes succeed on every prompt from models' red-teaming evaluation sets.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Scaling up model size inherently improves its resistance to adversarial jailbreak attacks.", label: "refute", status: null }
                ]
            },
            prolificdreamer: {
                paper: {
                    title: "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation",
                    publicationDate: "2023-05-25",
                    url: "https://www.semanticscholar.org/paper/c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
                    abstract: "Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Variational score distillation (VSD) improves diversity and sample quality with a common CFG weight in text-to-3D generation.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "SDS is suitable for real-time applications due to its computational efficiency and low resource requirements.", label: "refute", status: null }
                ]
            },
            flow_matching: {
                paper: {
                    title: "Flow Matching for Generative Modeling",
                    publicationDate: "2022-10-06",
                    url: "https://www.semanticscholar.org/paper/af68f10ab5078bfc519caae377c90ee6d9c504e9",
                    abstract: "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Flow Matching provides more efficient training and better generalization for CNFs compared to diffusion paths.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Flow Matching improves the interpretability of generated images compared to traditional generative modeling techniques.", label: "refute", status: null }
                ]
            },
            galactica: {
                paper: {
                    title: "Galactica: A Large Language Model for Science",
                    publicationDate: "2022-11-16",
                    url: "https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1",
                    abstract: "Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Galactica outperforms GPT-3 by 68.2% versus 49.0% on technical knowledge probes such as LaTeX equations.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Galactica can fully replace human researchers in scientific literature review and knowledge discovery tasks.", label: "refute", status: null }
                ]
            },
            bloomberggpt: {
                paper: {
                    title: "BloombergGPT: A Large Language Model for Finance",
                    publicationDate: "2023-03-30",
                    url: "https://www.semanticscholar.org/paper/83edcfbb206ddad38a971d605da09390604248ea",
                    abstract: "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "BloombergGPT outperforms existing models on financial tasks by significant margins without sacrificing general LLM performance.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "A domain-specific language model can effectively outperform BloombergGPT on both financial tasks and general benchmarks.", label: "refute", status: null }
                ]
            },
            lima: {
                paper: {
                    title: "LIMA: Less Is More for Alignment",
                    publicationDate: "2023-05-18",
                    url: "https://www.semanticscholar.org/paper/546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
                    abstract: "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Almost all knowledge in large language models is learned during pretraining, not during instruction tuning.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Instruction tuning with reinforcement learning has a significant impact on the model's ability to generalize well.", label: "refute", status: null }
                ]
            },
            voyager: {
                paper: {
                    title: "Voyager: An Open-Ended Embodied Agent with Large Language Models",
                    publicationDate: "2023-05-25",
                    url: "https://www.semanticscholar.org/paper/f197bf0fc2f228483f6af3285000d54d8d97f9eb",
                    abstract: "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Voyager is the first LLM-powered embodied lifelong learning agent in Minecraft without human intervention.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Voyager significantly accelerates the learning process for any game beyond Minecraft, outperforming other learning techniques consistently.", label: "refute", status: null }
                ]
            },
            mmbench: {
                paper: {
                    title: "MMBench: Is Your Multi-modal Model an All-around Player?",
                    publicationDate: "2023-07-12",
                    url: "https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed",
                    abstract: "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "MMBench is a systematic objective benchmark enabling comprehensive bilingual evaluation of vision-language models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "MMBench improves the performance of vision-language models beyond existing benchmarks in real-world applications.", label: "refute", status: null }
                ]
            },
            spanish_bert: {
                paper: {
                    title: "Spanish Pre-trained BERT Model and Evaluation Data",
                    publicationDate: "2023-08-06",
                    url: "https://www.semanticscholar.org/paper/79926aa63d4daee6af06a8e9a7c2480b31cb7ed9",
                    abstract: "The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "A BERT-based language model pre-trained exclusively on Spanish data achieves state-of-the-art results on benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Our model consistently outperforms all other language models across every Spanish language task tested.", label: "refute", status: null }
                ]
            },
            llm_agents_survey: {
                paper: {
                    title: "The Rise and Potential of Large Language Model Based Agents: A Survey",
                    publicationDate: "2023-09-14",
                    url: "https://www.semanticscholar.org/paper/0c72450890a54b68d63baa99376131fda8f06cf9",
                    abstract: "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Large language models (LLMs) are suitable foundations for designing adaptable, general-purpose AI agents.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLM-based agents fully understand and mimic human emotions in a variety of complex social interactions.", label: "refute", status: null }
                ]
            },
            program_of_thoughts: {
                paper: {
                    title: "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                    publicationDate: "2022-11-22",
                    url: "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691",
                    abstract: "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "PoT improves average performance over CoT by around 12% on evaluated math and financial datasets.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "PoT method improves language model reasoning capabilities beyond numerical tasks, enabling advancements in textual inference contexts.", label: "refute", status: null }
                ]
            },
            baichuan2: {
                paper: {
                    title: "Baichuan 2: Open Large-scale Language Models",
                    publicationDate: "2023-09-19",
                    url: "https://www.semanticscholar.org/paper/c96297261467b5daa2d01227496a70d444602434",
                    abstract: "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Baichuan 2 excels in vertical domains such as medicine and law.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Baichuan 2's architecture introduces novel mechanisms for zero-shot learning beyond existing methods.", label: "refute", status: null }
                ]
            },
            chatgpt_medicine: {
                paper: {
                    title: "ChatGPT in medicine: an overview of its applications, advantages, limitations, future prospects, and ethical considerations",
                    publicationDate: "2023-05-04",
                    url: "https://www.semanticscholar.org/paper/465b0e9fbca264713b5904c3084f8c8c6e1039b1",
                    abstract: "This paper presents an analysis of the advantages, limitations, ethical considerations, future prospects, and practical applications of ChatGPT and artificial intelligence (AI) in the healthcare and medical domains. ChatGPT is an advanced language model that uses deep learning techniques to produce human-like responses to natural language inputs. It is part of the family of generative pre-training transformer (GPT) models developed by OpenAI and is currently one of the largest publicly available language models. ChatGPT is capable of capturing the nuances and intricacies of human language, allowing it to generate appropriate and contextually relevant responses across a broad spectrum of prompts. The potential applications of ChatGPT in the medical field range from identifying potential research topics to assisting professionals in clinical and laboratory diagnosis. Additionally, it can be used to help medical students, doctors, nurses, and all members of the healthcare fraternity to know about updates and new developments in their respective fields. The development of virtual assistants to aid patients in managing their health is another important application of ChatGPT in medicine. Despite its potential applications, the use of ChatGPT and other AI tools in medical writing also poses ethical and legal concerns. These include possible infringement of copyright laws, medico-legal complications, and the need for transparency in AI-generated content. In conclusion, ChatGPT has several potential applications in the medical and healthcare fields. However, these applications come with several limitations and ethical considerations which are presented in detail along with future prospects in medicine and healthcare."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "ChatGPT can aid patients in managing their health through the development of virtual assistants.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ChatGPT can replace doctors in diagnosing and treating patients independently without human supervision.", label: "refute", status: null }
                ]
            },
            llm_musculoskeletal: {
                paper: {
                    title: "Impact of artificial intelligence in managing musculoskeletal pathologies in physiatry: a qualitative observational study evaluating the potential use of ChatGPT versus Copilot for patient information and clinical advice on low back pain.",
                    publicationDate: "2024-11-29",
                    url: "https://www.semanticscholar.org/paper/0a4ecfe695c907f3f1af018d73ac81f78d2f7bf6",
                    abstract: "Background\nThe self-management of low back pain (LBP) through patient information interventions offers significant benefits in terms of cost, reduced work absenteeism, and overall healthcare utilization. Using a large language model (LLM), such as ChatGPT (OpenAI) or Copilot (Microsoft), could potentially enhance these outcomes further. Thus, it is important to evaluate the LLMs ChatGPT and Copilot in providing medical advice for LBP and assessing the impact of clinical context on the quality of responses.\n\n\nMethods\nThis was a qualitative comparative observational study. It was conducted within the Department of Physical Medicine and Rehabilitation, University of Montreal in Montreal, QC, Canada. ChatGPT and Copilot were used to answer 27 common questions related to LBP, with and without a specific clinical context. The responses were evaluated by physiatrists for validity, safety, and usefulness using a 4-point Likert scale (4, most favorable).\n\n\nResults\nBoth ChatGPT and Copilot demonstrated good performance across all measures. Validity scores were 3.33 for ChatGPT and 3.18 for Copilot, safety scores were 3.19 for ChatGPT and 3.13 for Copilot, and usefulness scores were 3.60 for ChatGPT and 3.57 for Copilot. The inclusion of clinical context did not significantly change the results.\n\n\nConclusion\nLLMs, such as ChatGPT and Copilot, can provide reliable medical advice on LBP, irrespective of the detailed clinical context, supporting their potential to aid in patient self-management."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "LLMs, like ChatGPT and Copilot, can reliably provide medical advice on LBP regardless of clinical context.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLMs can replace healthcare professionals in providing medical assessments and treatment plans for patients with LBP.", label: "refute", status: null }
                ]
            },
            starcoder: {
                paper: {
                    title: "StarCoder: may the source be with you!",
                    publicationDate: "2023-05-09",
                    url: "https://www.semanticscholar.org/paper/78281482c1fdad8e167bab39cc9955c73d58ae8f",
                    abstract: "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infill capabilities, and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms models such as PaLM, LaMDA, and LLaMA, being more than 40% smaller than the next-largest open model. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline, a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model License."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches OpenAI code-cushman-001.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "StarCoder models can generalize equally well across all programming languages without any performance degradation.", label: "refute", status: null }
                ]
            },
            faun_eval: {
                paper: {
                    title: "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models",
                    publicationDate: "2024-11-27",
                    url: "https://www.semanticscholar.org/paper/75ed9489c848d743a051fc088d16c23dcd2b3b9b",
                    abstract: "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Top-performing LLMs differ across tasks of QA, fault localization, and code editing in software issue resolution.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "LLMs can autonomously resolve software issues without any human intervention.", label: "refute", status: null }
                ]
            },
            eva: {
                paper: {
                    title: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
                    publicationDate: "2022-11-14",
                    url: "https://www.semanticscholar.org/paper/78281482c1fdad8e167bab39cc9955c73d58ae8f",
                    abstract: "We launch EVA, a vision-centric foundation model to Explore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "EVA achieves almost state-of-the-art performance on LVIS dataset with over a thousand categories.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "EVA facilitates seamless integration between visual and auditory data for enhanced multi-modal learning capabilities.", label: "refute", status: null }
                ]
            },
            literalgraph: {
                paper: {
                    title: "Building a literature knowledge base towards transparent biomedical AI",
                    publicationDate: "2025-01-28",
                    url: "https://www.semanticscholar.org/paper/3b6179c293df29e31d31cea46476f104ab6950f2",
                    abstract: "As artificial intelligence (AI) continues to advance and scale up in biomedical research, concerns about AI's trustworthiness and transparency have grown. There is a critical need to systematically bring accurate and relevant biomedical knowledge into AI applications for transparency and provenance. Knowledge graphs have emerged as a powerful tool that integrates heterogeneous knowledge by explicitly describing biomedical knowledge as entities and relationships between entities. However, PubMed, the largest and most comprehensive repository of biomedical knowledge, exists primarily as unstructured text and is under utilized for advanced machine learning tasks. To address the challenge, we developed LiteralGraph, a computational framework to extract biomedical terms and relationships from PubMed literature into a unified knowledge graph. Using this framework, we established the Genomic Literature Knowledge Base (GLKB), which consolidates 14,634,427 biomedical relationships between 3,276,336 biomedical terms from over 33 million PubMed abstracts and nine well-established biomedical repositories. The database is coupled with RESTful APIs and a user-friendly web interface that makes it accessible to researchers for various usages. We demonstrated the broad utility of GLKB towards transparent AI in three distinct application scenarios. In the LLM grounding scenario, we developed a Retrieval Augmented Generation (RAG) agent to reduce LLM hallucination in biomedical question answering. In the hypothesis generation scenario, we elucidated the potential functions of RFX6 in type 2 diabetes (T2D) using the vast evidence from PubMed articles. In the machine learning scenario, we utilized GLKB to provide semantic knowledge in predictive tasks and scientific fact-checking."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "We developed LiteralGraph to systematically extract and consolidate biomedical relationships from PubMed into a knowledge graph.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Knowledge graphs are the ultimate solution for all trustworthiness issues in AI biomedical applications.", label: "refute", status: null }
                ]
            },
            kosmos2: {
                paper: {
                    title: "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                    publicationDate: "2023-06-26",
                    url: "https://www.semanticscholar.org/paper/3b6179c293df29e31d31cea46476f104ab6950f2",
                    abstract: "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Kosmos-2 integrates grounding capabilities into multimodal large language models for improved downstream applications.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Kosmos-2 demonstrates artificial general intelligence through its integration of multimodal grounding capabilities into downstream applications.", label: "refute", status: null }
                ]
            },
            lmseg: {
                paper: {
                    title: "LMSeg: Unleashing the Power of Large-Scale Models for Open-Vocabulary Semantic Segmentation",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/10c1b8e4d842c915027fb3ed61e9596c8fa8ff00",
                    abstract: "It is widely agreed that open-vocabulary-based approaches outperform classical closed-set training solutions for recognizing unseen objects in images for semantic segmentation. Existing open-vocabulary approaches leverage vision-language models, such as CLIP, to align visual features with rich semantic features acquired through pre-training on large-scale vision-language datasets. However, the text prompts employed in these methods are short phrases based on fixed templates, failing to capture comprehensive object attributes. Moreover, while the CLIP model excels at exploiting image-level features, it is less effective at pixel-level representation, which is crucial for semantic segmentation tasks. In this work, we propose to alleviate the above-mentioned issues by leveraging multiple large-scale models to enhance the alignment between fine-grained visual features and enriched linguistic features. Specifically, our method employs large language models (LLMs) to generate enriched language prompts with diverse visual attributes for each category, including color, shape/size, and texture/material. Additionally, for enhanced visual feature extraction, the SAM model is adopted as a supplement to the CLIP visual encoder through a proposed learnable weighted fusion strategy. Built upon these techniques, our method, termed LMSeg, achieves state-of-the-art performance across all major open-vocabulary segmentation benchmarks. The code will be made available soon."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "LMSeg achieves state-of-the-art performance across all major open-vocabulary segmentation benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Open-vocabulary-based approaches completely eliminate the need for any closed-set training solutions in segmentation tasks.", label: "refute", status: null }
                ]
            },
            dreamdance: {
                paper: {
                    title: "DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/0be2accfc866bbe4aa1d507b0d2b53cd5fc4a62a",
                    abstract: "In this work, we present DreamDance, a novel method for animating human images using only skeleton pose sequences as conditional inputs. Existing approaches struggle with generating coherent, high-quality content in an efficient and user-friendly manner. Concretely, baseline methods relying on only 2D pose guidance lack the cues of 3D information, leading to suboptimal results, while methods using 3D representation as guidance achieve higher quality but involve a cumbersome and time-intensive process. To address these limitations, DreamDance enriches 3D geometry cues from 2D poses by introducing an efficient diffusion model, enabling high-quality human image animation with various guidance. Our key insight is that human images naturally exhibit multiple levels of correlation, progressing from coarse skeleton poses to fine-grained geometry cues, and further from these geometry cues to explicit appearance details. Capturing such correlations could enrich the guidance signals, facilitating intra-frame coherency and inter-frame consistency. Specifically, we construct the TikTok-Dance5K dataset, comprising 5K high-quality dance videos with detailed frame annotations, including human pose, depth, and normal maps. Next, we introduce a Mutually Aligned Geometry Diffusion Model to generate fine-grained depth and normal maps for enriched guidance. Finally, a Cross-domain Controller incorporates multi-level guidance to animate human images effectively with a video diffusion model. Extensive experiments demonstrate that our method achieves state-of-the-art performance in animating human images."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "DreamDance achieves state-of-the-art performance in animating human images from skeleton pose sequences.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "DreamDance can animate non-human images with the same effectiveness as it does human images.", label: "refute", status: null }
                ]
            },
            flan_collection: {
                paper: {
                    title: "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
                    publicationDate: "2023-01-31",
                    url: "https://www.semanticscholar.org/paper/f2b0017ddd77fa38760a18145e63553105a1a236",
                    abstract: "We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Training with mixed prompt settings yields stronger performance (2%+) in all evaluation settings for Flan-T5.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Flan-T5 demonstrates significant improvements specifically due to increased model size rather than design decisions.", label: "refute", status: null }
                ]
            },
            tarot: {
                paper: {
                    title: "TAROT: Targeted Data Selection via Optimal Transport",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/b5135224a7458dd8604d31d452fb4f31bdde452a",
                    abstract: "We propose TAROT, a targeted data selection framework grounded in optimal transport theory. Previous targeted data selection methods primarily rely on influence-based greedy heuristics to enhance domain-specific performance. While effective on limited, unimodal data (i.e., data following a single pattern), these methods struggle as target data complexity increases. Specifically, in multimodal distributions, these heuristics fail to account for multiple inherent patterns, leading to suboptimal data selection. This work identifies two primary factors contributing to this limitation: (i) the disproportionate impact of dominant feature components in high-dimensional influence estimation, and (ii) the restrictive linear additive assumptions inherent in greedy selection strategies. To address these challenges, TAROT incorporates whitened feature distance to mitigate dominant feature bias, providing a more reliable measure of data influence. Building on this, TAROT uses whitened feature distance to quantify and minimize the optimal transport distance between the selected data and target domains. Notably, this minimization also facilitates the estimation of optimal selection ratios. We evaluate TAROT across multiple tasks, including semantic segmentation, motion prediction, and instruction tuning. Results consistently show that TAROT outperforms state-of-the-art methods, highlighting its versatility across various deep learning tasks. Code is available at https://github.com/vita-epfl/TAROT."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "TAROT consistently outperforms state-of-the-art methods across various deep learning tasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "TAROT outperforms state-of-the-art methods in natural language processing tasks.", label: "refute", status: null }
                ]
            },
            mme: {
                paper: {
                    title: "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
                    publicationDate: "2023-06-23",
                    url: "https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981",
                    abstract: "Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Presenting the first comprehensive MLLM Evaluation benchmark measuring perception and cognition abilities on 14 subtasks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "MLLMs exhibit human-equivalent understanding and interpretation across all 14 evaluated subtasks in every scenario.", label: "refute", status: null }
                ]
            },
            atp_llava: {
                paper: {
                    title: "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/134a66cc64344b9ef913e0f25c2fab6f4ff92c94",
                    abstract: "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have identified redundancy in visual tokens within the Large Language Model (LLM) decoder layers and have mitigated this by pruning tokens using a pre-defined or fixed ratio, thereby reducing computational overhead. Nonetheless, we observe that the impact of pruning ratio varies across different LLM layers and instances (image-prompt pairs). Therefore, it is essential to develop a layer-wise and instance-wise vision token pruning strategy to balance computational cost and model performance effectively. We propose ATP-LLaVA, a novel approach that adaptively determines instance-specific token pruning ratios for each LLM layer. Specifically, we introduce an Adaptive Token Pruning (ATP) module, which computes the importance score and pruning threshold based on input instance adaptively. The ATP module can be seamlessly integrated between any two LLM layers with negligible computational overhead. Additionally, we develop a Spatial Augmented Pruning (SAP) strategy that prunes visual tokens with both token redundancy and spatial modeling perspectives. Our approach reduces the average token count by 75% while maintaining performance, with only a minimal 1.9% degradation across seven widely used benchmarks. The project page can be accessed via https://yxxxb.github.io/ATP-LLaVA-page/."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "ATP-LLaVA reduces visual token count by 75% with minimal 1.9% performance degradation across seven benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ATP-LLaVA improves robustness to adversarial attacks on vision-language models in addition to reducing computational cost.", label: "refute", status: null }
                ]
            },
            evalplus: {
                paper: {
                    title: "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
                    publicationDate: "2023-05-02",
                    url: "https://www.semanticscholar.org/paper/b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a",
                    abstract: "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "HumanEval+ reduces pass@k by up to 19.3-28.9% for LLM-synthesized code compared to the original HumanEval.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Automated test-case generation fully eliminates the need for human-written test-cases in evaluating LLM-synthesized code.", label: "refute", status: null }
                ]
            },
            vall_e: {
                paper: {
                    title: "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
                    publicationDate: "2023-01-05",
                    url: "https://www.semanticscholar.org/paper/c2f91f35df893714418cc29096083dce0b441229",
                    abstract: "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 50 k hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capability and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment from the prompt in synthesis."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "VALL-E significantly outperforms state-of-the-art zero-shot TTS systems in speech naturalness and speaker similarity.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "VALL-E reduces computational requirements and training time compared to previous TTS systems.", label: "refute", status: null }
                ]
            },
            scaling_transformers_speech: {
                paper: {
                    title: "Scaling Transformers for Low-Bitrate High-Quality Speech Coding",
                    publicationDate: "2024-11-29",
                    url: "https://www.semanticscholar.org/paper/4d46eb61b055a603242cd6ae220c47540ef1be9d",
                    abstract: "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Scaling a transformer architecture with FSQ achieves state-of-the-art speech quality at 400 or 700 bps.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The model effectively tokenizes non-speech audio such as music or environmental sounds at low bit-rates.", label: "refute", status: null }
                ]
            },
            voiceprompter: {
                paper: {
                    title: "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching",
                    publicationDate: "2025-01-29",
                    url: "https://www.semanticscholar.org/paper/e2d4110e1bb045fce1755cf2fe00bcfb3fea1ba7",
                    abstract: "Despite remarkable advancements in recent voice conversion (VC) systems, enhancing speaker similarity in zero-shot scenarios remains challenging. This challenge arises from the difficulty of generalizing and adapting speaker characteristics in speech within zero-shot environments, which is further complicated by mismatch between the training and inference processes. To address these challenges, we propose VoicePrompter, a robust zero-shot VC model that leverages in-context learning with voice prompts. VoicePrompter is composed of (1) a factorization method that disentangles speech components and (2) a DiT-based conditional flow matching (CFM) decoder that conditions on these factorized features and voice prompts. Additionally, (3) latent mixup is used to enhance in-context learning by combining various speaker features. This approach improves speaker similarity and naturalness in zero-shot VC by applying mixup to latent representations. Experimental results demonstrate that VoicePrompter outperforms existing zero-shot VC systems in terms of speaker similarity, speech intelligibility, and audio quality. Our demo is available at \\url{https://hayeong0.github.io/VoicePrompter-demo/}."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "VoicePrompter improves speaker similarity and naturalness in zero-shot voice conversion using in-context learning with voice prompts.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "VoicePrompter significantly reduces computational costs while maintaining high performance in zero-shot voice conversion scenarios.", label: "refute", status: null }
                ]
            },
            ai_companion_ridehailing: {
                paper: {
                    title: "AI Companions in Ride-Hailing: Enhancing Sense of Safety with Voice Chatbots",
                    publicationDate: "2024-11-30",
                    url: "https://www.semanticscholar.org/paper/fd8df443f78d422da7ab5383c16f4d0a8f1f9718",
                    abstract: "In this paper, we present the development and formative evaluation of AI Companion, a voice chatbot based on Artificial Intelligence Large Language Models designed to enhance users' perception of safety and control during transportation using ride-hailing apps or similar services. We conducted a study with 10 participants to understand their travel experiences and evaluate the AI Companion's ability to maintain coherent conversations, simulating a phone call. Our findings indicate that participants generally liked the system. However, we provide several recommendations for significant improvements to the AI Companion."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "AI Companion can enhance users' perception of safety and control during transportation using ride-hailing apps.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The AI Companion significantly reduces crime rates in ride-hailing services.", label: "refute", status: null }
                ]
            },
            convnext_v2: {
                paper: {
                    title: "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
                    publicationDate: "2023-01-02",
                    url: "https://www.semanticscholar.org/paper/2218f1713d7f721ab76801063416ec9b11c7646f",
                    abstract: "Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt [33], have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE) [14]. However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "ConvNeXt V2 with GRN significantly enhances ConvNet performance in ImageNet, COCO, and ADE20K benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "ConvNeXt V2 models significantly outperform all other architectures on every visual recognition benchmark available.", label: "refute", status: null }
                ]
            },
            fasternet: {
                paper: {
                    title: "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",
                    publicationDate: "2023-03-07",
                    url: "https://www.semanticscholar.org/paper/a3aa1323a7f08c40207eaa359041e5bd72b25b27",
                    abstract: "To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of re-duction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise con-volution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-lk, our tiny FasterNet-TO is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher inference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at https://github.com/JierunChen/FasterNet."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Our tiny FasterNet-TO is 2.8×, 3.3×, and 2.4× faster than MobileViT-XXS on GPU, CPU, ARM respectively.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "FasterNet shows superior performance and efficiency for non-vision tasks compared to other neural networks.", label: "refute", status: null }
                ]
            },
            yolov7_spsd: {
                paper: {
                    title: "Detection of Critical Parts of River Crab Based on Lightweight YOLOv7-SPSD",
                    publicationDate: "2024-11-28",
                    url: "https://www.semanticscholar.org/paper/fa36cbaaae059563459027b29538f9080bc9622d",
                    abstract: "The removal of back armor marks the first stage in the comprehensive processing of river crabs. However, the current low level of mechanization undermines the effectiveness of this process. By integrating robotic systems with image recognition technology, the efficient removal of dorsal armor from river crabs is anticipated. This approach relies on the rapid identification and precise positioning of the processing location at the crab's tail, both of which are essential for optimal results. In this paper, we propose a lightweight deep learning model called YOLOv7-SPSD for detecting river crab tails. The goal is to accurately determine the processing location for the robotic removal of river crab back armor. We start by constructing a crab tail dataset and completing the data labeling process. To enhance the lightweight nature of the YOLOv7-tiny model, we incorporate the Slimneck module, PConv, and the SimAM attention mechanism. These additions help achieve an initial reduction in model size while preserving detection accuracy. Furthermore, we optimize the model by removing redundant parameters using the DepGraph pruning algorithm, which facilitates its application on edge devices. Experimental results show that the lightweight YOLOv7-SPSD model achieves a mean Average Precision (mAP) of 99.6% at a threshold of 0.5, an F1-score of 99.6%, and processes frames at a rate of 7.1 frames per second (FPS) on a CPU. Compared to YOLOv7-tiny, the improved model increases FPS by 2.7, reduces GFLOPS by 74.6%, decreases the number of parameters by 71.6%, and lowers its size by 8.1 MB. This study enhances the deployment of models in river crab processing equipment and introduces innovative concepts and methodologies for advancing intelligent river crab deep processing technology."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "YOLOv7-SPSD model achieves 99.6% mean Average Precision and F1-score for detecting river crab tails.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The model can be effectively utilized for detecting and processing other types of seafood automatically.", label: "refute", status: null }
                ]
            },
            improved_yolov8n_apple: {
                paper: {
                    title: "Real-Time Accurate Apple Detection Based on Improved YOLOv8n in Complex Natural Environments",
                    publicationDate: "2025-01-25",
                    url: "https://www.semanticscholar.org/paper/be880ac9229c2de0d8abace36913ccbb8c88e8bd",
                    abstract: "Efficient and accurate apple detection is crucial for the operation of apple-picking robots. To improve detection accuracy and speed, we propose a lightweight apple-detection model based on the YOLOv8n framework. The proposed model introduces a novel Self-Calibrated Coordinate (SCC) attention module, which enhances feature extraction, especially for partially occluded apples, by effectively capturing spatial and channel information. Additionally, we replace the C2f module within the YOLOv8n neck with a Partial Convolution Module improved with Reparameterization (PCMR), which accelerates detection, reduces redundant computations, and minimizes both parameter count and memory access during inference. To further optimize the model, we fuse multi-scale features from the second and third pyramid levels of the backbone architecture, achieving a lightweight design suitable for real-time detection. To address missed detections and misclassifications, Polynomial Loss (PolyLoss) is integrated, enhancing class discrimination for different apple subcategories. Compared to the original YOLOv8n, the improved model increases the mAP by 2.90% to 88.90% and improves the detection speed to 220 FPS, which is 30.55% faster. Additionally, it reduces the parameter count by 89.36% and the FLOPs by 2.47%. Experimental results demonstrate that the proposed model outperforms mainstream object-detection algorithms, including Faster R-CNN, RetinaNet, SSD, RT-DETR-R18, RT-DETR-R34, YOLOv5n, YOLOv6-N, YOLOv7-tiny, YOLOv8n, YOLOv9-T and YOLOv11n, in both mAP and detection speed. Notably, the improved model has been used to develop an Android application deployed on the iQOO Neo6 SE smartphone, achieving a 40 FPS detection speed, a 26.93% improvement over the corresponding deployment of YOLOv8n, enabling real-time apple detection. This study provides a valuable reference for designing efficient and lightweight detection models for resource-constrained apple-picking robots."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "The proposed model increases the mAP by 2.90% to 88.90% and detection speed to 220 FPS.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "The model's performance significantly improves general object detection accuracy beyond apple detection in diverse environments.", label: "refute", status: null }
                ]
            },
            baichuan_omni: {
                paper: {
                    title: "Baichuan-Omni-1.5 Technical Report",
                    publicationDate: "2025-01-26",
                    url: "https://www.semanticscholar.org/paper/f56928470abbe177dd6f7527523b54962fd465ed",
                    abstract: "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Baichuan-Omni-1.5 achieves results comparable to leading models across various multimodal medical benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Baichuan-Omni-1.5 surpasses human performance in all tested multimodal interaction tasks.", label: "refute", status: null }
                ]
            },
            self_debugging: {
                paper: {
                    title: "Teaching Large Language Models to Self-Debug",
                    publicationDate: "2023-04-11",
                    url: "https://www.semanticscholar.org/paper/9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
                    abstract: "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "Self-Debugging achieves state-of-the-art performance on several code generation benchmarks without human feedback.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Self-Debugging can eliminate the need for human programmers in all complex programming tasks.", label: "refute", status: null }
                ]
            },
            planning_driven_programming: {
                paper: {
                    title: "Planning-Driven Programming: A Large Language Model Programming Workflow",
                    publicationDate: "2024-11-21",
                    url: "https://www.semanticscholar.org/paper/d901b765310bbaacacae8b525ebc0b38a1ded087",
                    abstract: "The strong performance of large language models (LLMs) raises extensive discussion on their application to code generation. Recent research suggests continuous program refinements through visible tests to improve code generation accuracy in LLMs. However, these methods suffer from LLMs' inefficiency and limited reasoning capacity. In this work, we propose an LLM programming workflow (LPW) designed to improve both initial code generation and subsequent refinements within a structured two-phase workflow. Specifically, the solution generation phase formulates a solution plan, which is then verified through visible tests to specify the intended natural language solution. Subsequently, the code implementation phase drafts an initial code according to the solution plan and its verification. If the generated code fails the visible tests, the plan verification serves as the intended solution to consistently inform the refinement process for correcting bugs. Compared to state-of-the-art methods across various existing LLMs, LPW significantly improves the Pass@1 accuracy by up to 16.4% on well-established text-to-code generation benchmarks. LPW also sets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8% on MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using GPT-4o as the backbone."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "LPW significantly improves Pass@1 accuracy by up to 16.4% on text-to-code generation benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "One-shot code generation by LLMs always outperforms iterative refinement-based approaches in terms of accuracy and efficiency.", label: "refute", status: null }
                ]
            },
            instruction_tuning_gpt4: {
                paper: {
                    title: "Instruction Tuning with GPT-4",
                    publicationDate: "2023-04-06",
                    url: "https://www.semanticscholar.org/paper/9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
                    abstract: "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "GPT-4-generated instruction-following data leads to superior zero-shot performance compared to previous state-of-the-art models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "Finetuning large language models using only human-written instructions achieves better zero-shot performance than machine-generated data.", label: "refute", status: null }
                ]
            },
            ai_translation_industry: {
                paper: {
                    title: "Artificial intelligence contribution to translation industry: looking back and forward",
                    publicationDate: "2024-11-29",
                    url: "https://www.semanticscholar.org/paper/c18e8b94de7d009fc41e88c01f2520d06d07b40d",
                    abstract: "This study provides a comprehensive analysis of artificial intelligence (AI) contribution to translation industry (ACTI) research, synthesizing it over forty-one years from 1980-2024. 13220 articles were retrieved from three sources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz., scientometric and thematic, focusing on cluster, subject categories, keywords, burstness, centrality and research centers as for the former. For the latter, we thematically review 18 articles, selected purposefully from the articles involved, centering on purpose, approach, findings, and contribution to ACTI future directions. The findings reveal that in the past AI contribution to translation industry was not rigorous, resulting in rule-based machine translation and statistical machine translation whose output was not satisfactory. However, the more AI develops, the more machine translation develops, incorporating Neural Networking Algorithms and (Deep) Language Learning Models like ChatGPT whose translation output has developed considerably. However, much rigorous research is still needed to overcome several problems encountering translation industry, specifically concerning low-source languages, multi-dialectical and free word order languages, and cultural and religious registers."
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "The more AI develops, the more machine translation develops, incorporating Neural Networking Algorithms and Deep Learning Models.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "AI significantly reduces human translator employment in the translation industry.", label: "refute", status: null }
                ]
            },
            wizardcoder: {
                paper: {
                    title: "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                    publicationDate: "2023-06-14",
                    url: "https://www.semanticscholar.org/paper/454c8fef2957aa2fb13eb2c7a454393a2ee83805",
                    abstract: "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM"
                },
                claims: [
                    { id: "c1", source: "synthetic", text: "WizardCoder surpasses all other open-source Code LLMs by a substantial margin on code generation benchmarks.", label: "support", status: null },
                    { id: "c2", source: "synthetic", text: "WizardCoder consistently outperforms all closed-source LLMs across every code generation task and benchmark.", label: "refute", status: null }
                ]
            }
        };

        // 2) Helpers to read the current id and content
        function getIdFromURL() {
            const params = new URLSearchParams(window.location.search);
            return params.get("id") || "qlora"; // default if missing
        }
        function getContentById(id) {
            return CONTENT[id] || null;
        }

        // ClaimAnnotationApp React component
        function ClaimAnnotationApp({ initialPaper, initialClaims }) {
            const [paper] = React.useState(initialPaper);
            const [claims, setClaims] = React.useState(initialClaims);
            const [comments, setComments] = React.useState("");
            const [consent, setConsent] = React.useState(false);
            const [saving, setSaving] = React.useState("idle");
            const [draftAccepted, setDraftAccepted] = React.useState({
                mainSupported: null, // null = not selected, true = yes, false = no
                relevantUnsupported: null
            });

            const handleClaimChange = (index, value) => {
                const newClaims = [...claims];
                newClaims[index] = { ...newClaims[index], editedText: value, status: "edited" };
                setClaims(newClaims);
            };

            const handleAcceptClaim = (index, checked) => {
                const newClaims = [...claims];
                if (checked) {
                    newClaims[index] = { ...newClaims[index], status: "accepted", editedText: undefined };
                } else {
                    newClaims[index] = { ...newClaims[index], status: null };
                }
                setClaims(newClaims);
            };

            const handleDraftSelection = (claimType, accepted) => {
                setDraftAccepted(prev => ({
                    ...prev,
                    [claimType]: accepted
                }));
                
                // If they accept the draft, mark it as accepted
                const index = claimType === 'mainSupported' ? 0 : 1;
                const newClaims = [...claims];
                if (accepted) {
                    newClaims[index] = { ...newClaims[index], status: "accepted", editedText: undefined };
                } else {
                    newClaims[index] = { ...newClaims[index], status: null };
                }
                setClaims(newClaims);
            };

            const handleSubmit = async (e) => {
                e.preventDefault();
                if (!consent) return;

                const payload = {
                    paper, claims, comments, consent,
                    timestamp: new Date().toISOString(),
                };

                setSaving("saving");
                try {
                    await submitToSheet(payload);
                    setSaving("ok");
                } catch (err) {
                    setSaving("err");
                    downloadJSON(payload);
                }
            };

            return (
                <div className="min-h-screen bg-gray-50 text-gray-900">
                    <header className="border-b bg-white">
                        <div className="max-w-4xl mx-auto px-4 py-5 flex items-center gap-3">
                            <div className="h-9 w-9 rounded bg-purple-900 text-white grid place-items-center font-bold">UW</div>
                            <div>
                                <h1 className="text-lg font-semibold">Scientific Claim Annotation</h1>
                                <p className="text-sm text-gray-600">University of Washington</p>
                            </div>
                        </div>
                    </header>
                    
                    <main className="max-w-4xl mx-auto px-4 py-6">
                        {/* Overview Section */}
                        <section className="mb-6">
                            <h2 className="text-lg font-semibold text-gray-900 mb-2">Overview</h2>
                            <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                <p className="text-base leading-relaxed mb-2">Thank you for participating in our study!</p>
                                <p className="text-base mb-2">We invite you to provide two scientific claims about your paper:</p>
                                <ul className="list-disc list-inside text-base mb-2">
                                    <li><strong>Main supported claim</strong>: The main scientific claim that is uniquely supported by your paper's findings.</li>
                                    <li><strong>Relevant but unsupported claim</strong>: A scientific claim that is related to the topic of your paper, but which your paper does not support.</li>
                                </ul>
                                <p className="text-base mb-2"><i>Note</i>: A <u>scientific claim</u> is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source. Aim for approximately 20 words per claim.</p>
                                <p className="text-base mb-2">We draft both claims for you. Please feel free to edit, refine, or rewrite them as needed, or accept the draft claims if they fully satisfy the requirements.</p>
                                {/* <p className="text-sm text-gray-600 mt-2">A <u>scientific claim</u> is a atomic verifiable statements expressing a finding about one aspect of a scientific entity or process, which can be verified against a single source.</p> */}
                            </div>
                        </section>
                        
                        {/* Paper Information Section */}
                        <section className="mb-6">
                            <h2 className="text-lg font-semibold text-gray-900 mb-2">Paper Information</h2>
                            <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                <div className="flex flex-col sm:flex-row gap-2 py-1">
                                    <div className="sm:w-32 text-gray-600">Title</div>
                                    <div className="flex-1">
                                        <div className="font-medium">{paper.title}</div>
                                    </div>
                                </div>
                                <div className="flex flex-col sm:flex-row gap-2 py-1">
                                    <div className="sm:w-32 text-gray-600">Publication Date</div>
                                    <div className="flex-1">
                                        <div>{paper.publicationDate}</div>
                                    </div>
                                </div>
                                <div className="flex flex-col sm:flex-row gap-2 py-1">
                                    <div className="sm:w-32 text-gray-600">Link</div>
                                    <div className="flex-1">
                                        <div><a className="text-blue-700 underline" href={paper.url} target="_blank">Semantic Scholar</a></div>
                                    </div>
                                </div>
                                <div className="flex flex-col sm:flex-row gap-2 py-1">
                                    <div className="sm:w-32 text-gray-600">Abstract</div>
                                    <div className="flex-1">
                                        <p className="text-sm text-gray-800 leading-relaxed">{paper.abstract}</p>
                                    </div>
                                </div>
                            </div>
                        </section>
                        
                        {/* Main Supported Claim Section */}
                        <section className="mb-6">
                            <h2 className="text-lg font-semibold text-gray-900 mb-2">Main supported claim</h2>
                            <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                <div className="mb-6">
                                    {/* Show draft claim first */}
                                    <div className="mb-4">
                                        <p className="text-sm text-gray-600 mb-2">Draft claim:</p>
                                        <div className="bg-gray-50 border rounded p-3 mb-3">
                                            <p className="text-base text-gray-800">{claims[0]?.text}</p>
                                        </div>
                                        <div className="flex justify-end mb-3">
                                            <p className="text-xs text-gray-500">generated by GPT-4o </p>
                                        </div>
                                        
                            {/* Yes/No selection */}
                            <div className="mb-4">
                                <p className="text-base text-gray-700 mb-2">Is this the main claim that your paper uniquely supports?</p>
                                <div className="space-y-2">
                                    <label className="flex items-center gap-10">
                                        <input
                                            type="radio"
                                            name="mainSupportedDraft"
                                            className="h-4 w-4"
                                            checked={draftAccepted.mainSupported === true}
                                            onChange={() => handleDraftSelection('mainSupported', true)}
                                        />
                                        <span className="text-base text-gray-700">Yes</span>
                                    </label>
                                    <label className="flex items-center gap-10">
                                        <input
                                            type="radio"
                                            name="mainSupportedDraft"
                                            className="h-4 w-4"
                                            checked={draftAccepted.mainSupported === false}
                                            onChange={() => handleDraftSelection('mainSupported', false)}
                                        />
                                        <span className="text-base text-gray-700">No</span>
                                    </label>
                                </div>
                            </div>
                                    </div>
                                    
                                    {/* Show editable textarea only if they selected "No" */}
                                    {draftAccepted.mainSupported === false && (
                                        <div>
                                            <p className="text-sm text-gray-700 mb-2">Please provide your main supported claim:</p>
                                            <textarea
                                                className="w-full min-h-5rem rounded border p-2 text-gray-500 text-base"
                                                placeholder="Enter your main supported claim here..."
                                                value={claims[0]?.editedText || claims[0]?.text || ""}
                                                onChange={(e) => handleClaimChange(0, e.target.value)}
                                            />
                                        </div>
                                    )}
                                </div>
                            </div>
                        </section>
                        
                        {/* Relevant but Unsupported Claim Section */}
                        <section className="mb-6">
                            <h2 className="text-lg font-semibold text-gray-900 mb-2">Relevant but unsupported claim</h2>
                            <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                <div className="mb-6">
                                    {/* Show draft claim first */}
                                    <div className="mb-4">
                                        <p className="text-sm text-gray-600 mb-2">Draft claim generated by GPT-4o:</p>
                                        <div className="bg-gray-50 border rounded p-3 mb-3">
                                            <p className="text-base text-gray-800">{claims[1]?.text}</p>
                                        </div>
                                        <div className="flex justify-end mb-3">
                                            <p className="text-xs text-gray-500">generated by GPT-4o </p>
                                        </div>
                                        
                            {/* Yes/No selection */}
                            <div className="mb-4">
                                <p className="text-base text-gray-700 mb-2">Is this a relevant but unsupported claim?</p>
                                <div className="space-y-2">
                                    <label className="flex items-center gap-10">
                                        <input
                                            type="radio"
                                            name="relevantUnsupportedDraft"
                                            className="h-4 w-4"
                                            checked={draftAccepted.relevantUnsupported === true}
                                            onChange={() => handleDraftSelection('relevantUnsupported', true)}
                                        />
                                        <span className="text-base text-gray-700">Yes</span>
                                    </label>
                                    <label className="flex items-center gap-10">
                                        <input
                                            type="radio"
                                            name="relevantUnsupportedDraft"
                                            className="h-4 w-4"
                                            checked={draftAccepted.relevantUnsupported === false}
                                            onChange={() => handleDraftSelection('relevantUnsupported', false)}
                                        />
                                        <span className="text-base text-gray-700">No</span>
                                    </label>
                                </div>
                            </div>
                                    </div>
                                    
                                    {/* Show editable textarea only if they selected "No" */}
                                    {draftAccepted.relevantUnsupported === false && (
                                        <div>
                                            <p className="text-sm text-gray-700 mb-2">Please provide your relevant but unsupported claim:</p>
                                            <textarea
                                                className="w-full min-h-5rem rounded border p-2 text-gray-500 text-base"
                                                placeholder="Enter your relevant but unsupported claim here..."
                                                value={claims[1]?.editedText || claims[1]?.text || ""}
                                                onChange={(e) => handleClaimChange(1, e.target.value)}
                                            />
                                        </div>
                                    )}
                                </div>
                            </div>
                        </section>
                        
                        {/* Comments Section */}
                        <section className="mb-6">
                            <h2 className="text-lg font-semibold text-gray-900 mb-2">Comments (optional)</h2>
                            <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                <textarea 
                                    placeholder="Any feedback or questions?" 
                                    className="w-full min-h-4rem rounded border p-2 text-base"
                                    value={comments}
                                    onChange={(e) => setComments(e.target.value)}
                                />
                            </div>
                        </section>
                        
                        {/* Consent & Submit Section */}
                        <form onSubmit={handleSubmit} className="mb-10">
                            <section className="mb-6">
                                <h2 className="text-lg font-semibold text-gray-900 mb-2">Consent & Submit</h2>
                                <div className="bg-white border border-gray-200 rounded-xl p-4 shadow-sm">
                                    <div className="space-y-3">
                                        <div className="flex items-start gap-3">
                                            <input 
                                                type="checkbox" 
                                                className="mt-2 h-4 w-4" 
                                                checked={consent}
                                                onChange={(e) => setConsent(e.target.checked)}
                                            />
                                            <label className="text-base">I understand my annotations will be used for research purposes and I will not submit any sensitive or confidential information. I will be acknowledged for my contributions.</label>
                                        </div>
                                        <div className="flex gap-3 items-center">
                                            <button 
                                                type="submit" 
                                                className="px-4 py-2 rounded bg-gray-900 text-white hover:bg-black disabled:opacity-50 text-base"
                                                disabled={saving === "saving"}
                                            >
                                                {saving === "saving" ? "Submitting..." : "Submit"}
                                            </button>
                                            {saving === "ok" && <span className="text-sm text-green-700">Saved ✓</span>}
                                            {saving === "err" && <span className="text-sm text-amber-700">Couldn't reach server; downloaded a backup JSON.</span>}
                                        </div>
                                    </div>
                                </div>
                            </section>
                        </form>
                    </main>
                    
                    <footer className="border-t bg-white">
                        <div className="max-w-4xl mx-auto px-4 py-4 text-xs text-gray-600">© UW — Scientific Claim Annotation Study.</div>
                    </footer>
                </div>
            );
        }

        // 3) A small wrapper that reacts to URL changes (back/forward)
        function AppRouter() {
            const [id, setId] = React.useState(getIdFromURL());
            React.useEffect(() => {
                const onPop = () => setId(getIdFromURL());
                window.addEventListener("popstate", onPop);
                return () => window.removeEventListener("popstate", onPop);
            }, []);

            const cfg = getContentById(id);
            if (!cfg) {
                return (
                    <div className="max-w-4xl mx-auto px-4 py-10">
                        <div className="border rounded-lg p-4 bg-yellow-50 border-yellow-200 text-yellow-900">
                            Unknown id: <b>{id}</b>.
                        </div>
                    </div>
                );
            }
            return <ClaimAnnotationApp initialPaper={cfg.paper} initialClaims={cfg.claims} />;
        }

        // 4) Mount the router (instead of mounting ClaimAnnotationApp directly)
        const root = ReactDOM.createRoot(document.getElementById("root"));
        root.render(<AppRouter />);
    </script>

</body>
</html>
